# General improvements
Negative trait impl of Unpin, PhantomData to have static analysis on lifetimes for FFI with C pointers.

1. fix important todos (copy, free, spinlock thats locked forever or moving and an option )
3. testing (LD_LIBRARY_PATH? since cargo test does not use build.rs get cargo test through .cargo and rustc to work and other testing)
run the linter

-> figure out what the exception handler error is and see if i can run qemu shell commands (read the qemu bootup logs)
i have the student version of egos so this will be tough to run things
on qemu, i cannot write to disk

since spinlock wont work on riscv32i, try an rcu lock:
let guard = rcu::read_lock();
let pair = value.try_access_with_guard(&guard)?;
The more rusty way is to contain the guard and let the guard be dropped by RAII.
Use some Empty types?

# Debugging
gdb check 
valgrind check
## 3 memory bugs at the FFI boundary (encapsulation/decapsulation of pointers)
1. creating a new pointer instead of returning the original one
2. read persistence to a buffer 
2.5. For 1. and 2. added proper abstraction to a C pointer: a locked &mut ref
3. when returning malloc'ed pointers to C, C must take care to free them

# Fall 2023 CS4999
## Concurrency + full filesystem 
### Step 1
More realisitic filesystem (treedisk). Can use c2rust and then create safe wrappers. Incoporate into build system and existing test file. Get rid if mallocs

### Step 2
Adding IO concurrency to the filesystem (async io, callbacks). E.g. Make a read to the filesystem, but return the results asynchronously, and give the read call a callback to run when the result is returned.

Need to update the inode interface to allow async IO.
#### Rust for Egos/Linux
async, abstractions, build, coould write something directly in linux. i have to write all abstractions from scratch, but can take inspriation from the rust for linux project. make some OS component useful for an embedded system, but niche enough. along the flavor of a 9p networked filesystem.
See Rust for Linux async bindings, RFC locks.

### Step 3
Add a cache access layer (another filesystem layer) that is properly tuned so that it is appropriately updated on reads and writes. This cache layer is below the filesystem layer but above the disk layer.

### Step 4
Block access in parallel, locks may come in so you need to lock a piece of memory or a whole inode or some other granularity, sync comes in as well. UI for user may still be sync (RW lock the inodes) (freelist needs its own lock).

### Step 5: Peripherals
RAID 0 and RAID 1 disk exists in egos 
init (read, write, getsize, setsize)
clockdisk
fatdisk
ramdisk/sddisk (sd driver readblock and writeblock interface (make the same interface))
make raid layers in rust
direction: disk drivers and a raid controller in Rust on top of the filesystem abstractions to see the benefits of static guarantees and state machines and encoding state in types with rust, sounds fancy, but hard to measure the benefits

or new drivers could use these platform crates for cortex-m or x86_64, dont need egos
pursue a research direction (papers) and using the riscv crates and extending egos
egos an easy to understand and extend os to ompl new ideas, like RAID or new drivers in Rust 
using HAL crates, cs4999, and using static guarantees from the embedded rust book and 
looking into more of the theory of rust to create something https://faultlore.com/blah/linear-rust/

RAID controllers disks built with HALS and state machines, get static guarantees on peripherals for proper configuration and access control
strict types for proper configuration of peripherals, access control
obrm + refernces for memory safety


# Aug 29
(real rust filesystem for egos)
1. refactoring and modularizing existing code to reuse
2. automation of porting c code to rust? not there yet...
3. rewriting, fatdisk with existing code simpler, asap, write tests as i go

# todo sep 5
generate an exhaustive test suite to test everything, full coverage, edge cases, list of operations to do read write block and inode 0, same and differnt blocks in inodes, every sequence of one operation, every sequence of two, every seq of three, and so on, compare with treedisk or in mem filesytem of array of bytes, small number of blocks?, we get teh shortest possible sequence from this method enumerating all cases
yunhao full solutions

# sep 12
1. TODO fix issue with compiling for make rust_apps rv32i, related to modules?
2. sanity check rust and c working together but
4. bindgen memory layout issues, the sizes from rust and c earth, grass, and inode_store, are not the same, the only problem is that inode_store because i interact with it
3. built-in rust test on x86
4. not quite done, but shouldnt take too much longer, the rest of them should be little logic bugs and edge cases
5. theres some related to the rust to c layout that arent passing and im a little stuck on
6. i have a couple hundred unit tests autogenerated
7. write the strings as inodes of varying block length, read them, check the metadata


print some info about the rust structs
"%d\n", (int) sizeof(struct xyz)
"%p\n", (&(struct xyz *) NULL) -> y
set offsets in rust 
#include <stdint.h> int32_t, but layout may still not be standardized
also change the compiler, where the 32bit vs 64bit addresses and alignemnt may be weird
24 vs 20 bytes
compiler favor execution efficiency on x86

TODO ensure portability assertions 24 bytes structs, learn about compiler things

#pragma pack(8) 
struct foobar {
  int x, int y, int z
}
don't put whitespace 

write a driver prgram that interprets comma separated string of writes and reads of strings for the exhaustive test, but first get the basics to work (sanity check)

# Last week of september
1. Implement a caching layer.
2. Clockdisk an lru cache.
3. Make a write-back cache.

1. Second chance algo different than clock algo.
2. I'm using a doubly linked list as a queue for the inodes in the cache, at that point I could just implement LRU its way more intutive, but apparently 
this is the exact reason an LRU cache is inefficient. So I'm supposed to use just raw memory.
3. Require some thought whether to use reference counting or just rust borrowing system to share the inodes if im using like high level data structures and sharing memory. Then the code is gonna be concurrent in the future as well.
4. CLock algo has a hand tht points to the last examined inode, which has a reference bit R which is 0, 1. If 0, put page there and advance hand. Otherwise, clear R, increment hand, and do so until we can replace the page.

# Pre-fall break Meeting notes
Clock for free, no LL, dirty bit on evict, make sure to writeback on evicttion with dirty dit, 20 layers, clocks, lrus, raid, debugging, synch()
RW LOCK the blcoks individually, or a big rw lock on the whole thing and then a marker on in progress
read(upcall), write(upcall lambda: write to the missing entry where the in progress block is) (innovation on the interface for IO parralelleism)
upcall based (async)
demo: constantly write to file block 0, synch, another app rading the same block over and over again (reader very slow in sync impl, reader unaffected by async)

# progress since fall break 
## impl caching layer 
write behind, synch will flush dirty blocks in write behind, synch(specific inode), sync(-1) flush all dirty blocks, clock algorithm for cache evicton
clock cache done, 
# TODO im working on theres some annoying design implementation details ,exhaustive testing working on a demo and benchmarks
# generally wrapping up and demoing the layering and synchronous filesystem stuff
## benchmarking?
flamegraphs, profiling 

## oct 17th progress
barriers: tokio doesn't work on embedded, theres a very basic mutex and CAS in the cortex-m crate, but it doesn't work on riscv32i, so i look at something in between from the rust for linux project

concurrency model
couple stages to extend the filesytem interface to be upcall based:
implement a reader writer lock, i need to atomically update and read the read and write counts, but one challenge is i think im limited to a spinlock for a mutex operation, in general a lot of the concurrency primitives are bugged in the riscv rust compiler, its very technical i dont really understand it

# cache stuff
benchmarking TODO
layering TODO
async operations future TODO async or concurrent 
cache blocks: layers: synch inode: write thru cache, wrrite behiond cache(synch
what does synch do?
sizse of cache? diminishing return
lrucache? clock strategy is approx for speed...
cc fs: io concurrency: rw lock on inodes 
layers..
impl caching layer 
both write thru and write behind
synch will flush dirty blocks in write behind 
synch(specific inode), sync(-1) flush all dirty blocks
fsync 
clock algorithm for cache evicton


# oct17th meeting notes
## opcodes
link and load for riscv or store and release opcalls
read and later store, read the value and only store if no one else changed the value in the meantime (store and release)
## coop multitasking
theres native async await syntax and passing functions around as upcalls, theres an embedded runtime for it called pasts, 

bag of upcalls (like event set) to upcalls to complete next, try this or try the other approach

otherwise upcalls are reentrant, and need to syncrhonize, so try different approaches, be careful of SYNC, one thread vs multiple threads, prioritize the upcalls? more difficult to code

rw lock on inodes
RW LOCK the blcoks individually, or a big rw lock on the whole thing and then a marker on in progress
read(upcall), write(upcall lambda: write to the missing entry where the in progress block is) (innovation on the interface for IO parralelleism)
upcall based (async)

# nov 9th progress
constrained by what libraries or existing design patterns that are already available for me because i dont think i have enough expertise to design my own things, riscv is a tier 2 platform so a lot of embedded libraries dont work that well or at all

start with something simple just to get an idea of the interface im gonna make
bag of upcalls, single thread, no sync

cooperative multitasking, an executor which will schedule the calls, concurrency comes in when
we need to wait on the io to complete, the executor will just schedule the next upcall to run, and the upcall will return a future, and the executor will run the future, and the future will return a value, and the upcall will return the value, and the executor will schedule the next upcall to run, and so on
(pasts crate)
mpsc channel is another approach, the io are the produceers and the single consumer is the ?  
see design note for more info

# post nov9th testing
constantly write to file block 0, synch, another app rading the same block over and over again (reader very slow in sync impl, reader unaffected by async)

# todo
get tested and working with riscv!

# nov 9th meeting